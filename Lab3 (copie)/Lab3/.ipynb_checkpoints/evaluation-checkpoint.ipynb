{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSTA Aishya Gabriel SE2\n",
    "\n",
    "\n",
    "DESCHAMPS Maxime     SE2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We have already seen 2 supervised learning methods in last lab classes. The Single Layer Perceptron and the Multi Layer Perceptron method. This time we will try to use another approach of a supervised learning, named Decision Trees (DT). This method consists of giving a predicted result on comparing different features of a dataset. We can prioritize one or more features at the beginning and move towards other features for comparison while training. We obtain a tree of neurons as a result for our supervised machine and we predict the test set results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we consider the  (binarized) Compas dataset that we studied in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: A decision tree configuration is a set of parameters that one can use to build decision trees. Propose 6 configurations that are likely to provide different topologies and caracteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree configuration is given by a set of parameters such as: splitter, max_depth and min_samples_leaf of the function DecisionTreeClassifier(). We can choose different attributes in order to have 6 different configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = tree.DecisionTreeClassifier(splitter='best', max_depth=None, min_samples_leaf=2)\n",
    "clf2 = tree.DecisionTreeClassifier(splitter='random', max_depth=None, min_samples_leaf=2)\n",
    "clf3= tree.DecisionTreeClassifier(splitter='best', max_depth=10, min_samples_leaf=3)\n",
    "clf4 = tree.DecisionTreeClassifier(splitter='best', max_depth=None, min_samples_leaf=4)\n",
    "clf5 = tree.DecisionTreeClassifier(splitter='best', max_depth=6, min_samples_leaf=2)\n",
    "clf6 = tree.DecisionTreeClassifier(splitter='random', max_depth=6, min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Train a decision tree for each of the previous configurations on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "\n",
    "train_examples, train_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "\n",
    "# First tree \n",
    "clf1 = clf1.fit(train_examples, train_labels)\n",
    "\n",
    "#fig = plt.figure(figsize=(300,40))\n",
    "#_ = tree.plot_tree(clf1, feature_names= (features), class_names= (prediction), filled=True, fontsize=10)\n",
    "\n",
    "#Second tree\n",
    "clf2 = clf2.fit(train_examples, train_labels)\n",
    "\n",
    "#fig = plt.figure(figsize=(300,40))\n",
    "#_ = tree.plot_tree(clf2, feature_names= (features), class_names= (prediction), filled=True, fontsize=10)\n",
    "\n",
    "#third tree\n",
    "clf3 = clf3.fit(train_examples, train_labels)\n",
    "\n",
    "#fig = plt.figure(figsize=(300,40))\n",
    "#_ = tree.plot_tree(clf3, feature_names= (features), class_names= (prediction), filled=True, fontsize=10)\n",
    "\n",
    "#fourth tree\n",
    "clf4 = clf4.fit(train_examples, train_labels)\n",
    "\n",
    "#fig = plt.figure(figsize=(300,40))\n",
    "#_ = tree.plot_tree(clf4, feature_names= (features), class_names= (prediction), filled=True, fontsize=10)\n",
    "\n",
    "#fifth tree\n",
    "clf5 = clf5.fit(train_examples, train_labels)\n",
    "\n",
    "#fig = plt.figure(figsize=(300,40))\n",
    "#_ = tree.plot_tree(clf5, feature_names= (features), class_names= (prediction), filled=True, fontsize=10)\n",
    "\n",
    "#sixth tree\n",
    "clf6 = clf6.fit(train_examples, train_labels)\n",
    "\n",
    "#fig = plt.figure(figsize=(300,40))\n",
    "#_ = tree.plot_tree(clf6, feature_names= (features), class_names= (prediction), filled=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Propose an evaluation in terms of training and testing accuracies using $5$-cross validation on two decision trees that have different topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6576613651313016\n",
      "0.6274881516587678\n",
      "0.6669064444856725\n",
      "0.6265402843601896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_examples, train_labels, test_size=0.20, random_state=0)\n",
    "\n",
    "#Decision Tree1\n",
    "scores_train_tree1 = cross_val_score(clf1, X_train, y_train, cv=5)\n",
    "scores_test_tree1 = cross_val_score(clf1, X_test, y_test, cv=5)\n",
    "print(scores_train_tree1.mean()) \n",
    "print(scores_test_tree1.mean())\n",
    "\n",
    "#6e arbre\n",
    "scores_train_tree6 = cross_val_score(clf6, X_train, y_train, cv=5)\n",
    "scores_test_tree6 = cross_val_score(clf6, X_test, y_test, cv=5)\n",
    "print(scores_train_tree6.mean())\n",
    "print(scores_test_tree6.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Propose an experimental study that shows the transition phase from underfitting to overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Construct the confusion matrix on a particular good configuration (after explaining your choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 143 184 332\n",
      "396 143 184 332\n",
      "382 157 181 335\n",
      "392 147 184 332\n",
      "367 172 167 349\n",
      "367 172 167 349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#DecisionArbre1\n",
    "y_predict1 = clf1.predict(X_test)\n",
    "[[TN1,FN1],[FP1,TP1]]=confusion_matrix(y_test, y_predict1)\n",
    "print(TN1,FN1,FP1,TP1)\n",
    "\n",
    "#DecisionArbre2\n",
    "y_predict2 = clf2.predict(X_test)\n",
    "[[TN2,FN2],[FP2,TP2]]=confusion_matrix(y_test, y_predict2)\n",
    "print(TN2,FN2,FP2,TP2)\n",
    "\n",
    "#DecisionArbre3\n",
    "y_predict3 = clf3.predict(X_test)\n",
    "[[TN3,FN3],[FP3,TP3]]=confusion_matrix(y_test, y_predict3)\n",
    "print(TN3,FN3,FP3,TP3)\n",
    "\n",
    "#DecisionArbre4\n",
    "y_predict4 = clf4.predict(X_test)\n",
    "[[TN4,FN4],[FP4,TP4]]=confusion_matrix(y_test, y_predict4)\n",
    "print(TN4,FN4,FP4,TP4)\n",
    "\n",
    "#DecisionArbre5\n",
    "y_predict5 = clf5.predict(X_test)\n",
    "[[TN5,FN5],[FP5,TP5]]=confusion_matrix(y_test, y_predict5)\n",
    "print(TN5,FN5,FP5,TP5)\n",
    "\n",
    "\n",
    "#DecisionArbre6\n",
    "y_predict6 = clf6.predict(X_test)\n",
    "[[TN6,FN6],[FP6,TP6]]=confusion_matrix(y_test, y_predict6)\n",
    "print(TN6,FN6,FP6,TP6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "The accuracy is given by TN+TP/(TN+TP+FN+FP). We want the best accuracy and also we want to take FP the least possible. That's why we would choose the tree 5 or 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Provide an evaluation of the fairness of the model based on the False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
